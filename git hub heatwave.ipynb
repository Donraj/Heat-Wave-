{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89792d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d497b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import files\n",
    "file_path='C:\\\\Users\\\\USER\\\\Desktop\\\\project\\\\analyzeddata\\\\'\n",
    "file_name=['OHI.nc','obsregrid.nc','UMO.nc','NCEP.nc','ECMWF.nc']\n",
    "file1,file2,file3,file4,file5=file_path+file_name[0],file_path+file_name[1],file_path+file_name[2],file_path+file_name[3],file_path+file_name[4]\n",
    "data1,data2,data3,data4,data5=nc.Dataset(file1),nc.Dataset(file2),nc.Dataset(file3),nc.Dataset(file4),nc.Dataset(file5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f589e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a date range and collect respective file data_time \n",
    "dates=pd.date_range(start='1/1/2000', end='31/12/2021')\n",
    "dsu=data1['time'].units\n",
    "dsu1=data2['time'].units\n",
    "dsu2=data3['time'].units\n",
    "dsu3=data4['time'].units\n",
    "dsu4=data5['time'].units\n",
    "data1_date=[nc.num2date(i,dsu,calendar='gregorian').strftime('%Y-%m-%d') for i in data1['time'][:]]\n",
    "data2_date=[nc.num2date(i,dsu1,calendar='gregorian').strftime('%Y-%m-%d') for i in data2['time'][:]]\n",
    "data3_date=[nc.num2date(i,dsu2,calendar='gregorian').strftime('%Y-%m-%d') for i in data3['time'][:]]\n",
    "data4_date=[nc.num2date(i,dsu3,calendar='gregorian').strftime('%Y-%m-%d') for i in data4['time'][:]]\n",
    "data5_date=[nc.num2date(i,dsu4,calendar='gregorian').strftime('%Y-%m-%d') for i in data5['time'][:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c851e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create recurring value between 1-30 for number of months in day\n",
    "pppp=[]\n",
    "for recday in range (0,len(data3_date),30):\n",
    "    pppp+=[np.arange(1,30)]\n",
    "tm=np.array(pppp).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8628afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3, ..., 27, 28, 29])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get dates that appear in all the data set\n",
    "li=[]\n",
    "for i in range (len(data1_date)):\n",
    "    if data1_date[i] in data5_date and data1_date[i] in data2_date and data1_date[i] in data3_date and data1_date[i] in data4_date:\n",
    "        li+=[data1_date[i]]\n",
    "\n",
    "#get index of the dates that appear in all the data in correspponding data\n",
    "lid1=[]\n",
    "for i in range (len(data1_date)):\n",
    "    if data1_date[i] in li:\n",
    "        lid1+=[i]\n",
    "lid2=[]\n",
    "for i in range (len(data2_date)):\n",
    "    if data2_date[i] in li:\n",
    "        lid2+=[i]\n",
    "lid3=[]\n",
    "for i in range (len(data3_date)):\n",
    "    if data3_date[i] in li:\n",
    "        lid3+=[i]\n",
    "lid4=[]\n",
    "for i in range (len(data4_date)):\n",
    "    if data4_date[i] in li:\n",
    "        lid4+=[i]\n",
    "lid5=[]\n",
    "for i in range (len(data5_date)):\n",
    "    if data5_date[i] in li:\n",
    "        lid5+=[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cf6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the time steps that corresponds to time period\n",
    "po=[]\n",
    "pq=[]\n",
    "for i in range(len(dates)):\n",
    "    if dates[i].month < 10 and dates[i].day < 10:\n",
    "        j=str(dates[i].year)+str('-')+str('0')+str(dates[i].month)+str('-')+str('0')+str(dates[i].day)#in li\n",
    "    elif dates[i].month > 10 and dates[i].day < 10:\n",
    "        j=str(dates[i].year)+str('-')+str(dates[i].month)+str('-')+str('0')+str(dates[i].day)\n",
    "    elif dates[i].month < 10 and dates[i].day > 10:\n",
    "        j=str(dates[i].year)+str('-')+str('0')+str(dates[i].month)+str('-')+str(dates[i].day)\n",
    "    elif dates[i].month > 10 and dates[i].day > 10:\n",
    "        j=str(dates[i].year)+str('-')+str(dates[i].month)+str('-')+str(dates[i].day)\n",
    "    if j in li:\n",
    "        po+=[i]\n",
    "        tr=data3_date.index(j)\n",
    "        tr=tm[tr]\n",
    "        pq+=[tr]\n",
    "po=pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9977c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip lon and lat\n",
    "jk=[]\n",
    "for i in data2['lon'][:]:\n",
    "    for m in data2['lat'][:]:\n",
    "        jk+=[zip([m],[i])]\n",
    "#unzip and collect into useable format\n",
    "latlon=[]\n",
    "for l in jk:\n",
    "    for k,m in l:\n",
    "        #ghg=str(k)+str(m)\n",
    "        latlon+=[(k,m)]\n",
    "        #print(k,m)\n",
    "##collect index of latlon\"\n",
    "for yu in latlon:\n",
    "    jh=np.array([13.8, 12.8, 11.8, 10.8])\n",
    "    kj=np.array([ 3.27,  4.27,  5.27,  6.27,  7.27,  8.27,  9.27, 10.27,11.27, 12.27, 13.27, 14.27])\n",
    "#data2['lat'][:]\n",
    "    for kkhk,g in enumerate(jh):\n",
    "        if str(g)==str(yu[0]):\n",
    "            break;\n",
    "    for mghg,g in enumerate(kj):\n",
    "        if str(g)== str(yu[1]):\n",
    "            break;\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get value from masked array\n",
    "df1=[]\n",
    "for ty in lid1:\n",
    "    df1+=[data1['temp'][ty,kkhk,mghg].item()]\n",
    "df2=[]\n",
    "for ty in lid2:\n",
    "    df2+=[data2['temp'][ty,kkhk,mghg].item()]\n",
    "df3=[]\n",
    "ttt3=[]\n",
    "for ty in lid3:\n",
    "    df3+=[data3['temp'][ty,kkhk,mghg].item()]\n",
    "df4=[]\n",
    "ttt4=[]\n",
    "for ty in lid4:\n",
    "    df4+=[data4['temp'][ty,kkhk,mghg].item()]\n",
    "df5=[]\n",
    "ttt5=[]\n",
    "for ty in lid5:\n",
    "    df5+=[data5['temp'][ty,kkhk,mghg].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile into dataframe\n",
    "dtt=pd.DataFrame()\n",
    "dtt['OBSHI']=df1\n",
    "dtt['OBS']=df2\n",
    "dtt['UMO']=df3\n",
    "dtt['NCEP']=df4\n",
    "dtt['ECMWF']=df5\n",
    "dtt['time lead']=po\n",
    "OBSHW=[]\n",
    "OBSHW_in=[]\n",
    "for i in range(len(dtt)):\n",
    "    if dtt['OBSHI'].iloc[i]>0:\n",
    "        k='H'\n",
    "        ku=1\n",
    "        OBSHW+=[k]\n",
    "        OBSHW_in+=[ku]\n",
    "    else:\n",
    "        k='NH'\n",
    "        ku=0\n",
    "        OBSHW+=[k]\n",
    "        OBSHW_in+=[ku]\n",
    "dtt['OBSHW']=OBSHW\n",
    "dtt['OBSHW_in']=OBSHW_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffc95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create gridpoint specific model for the whole region\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "i=0\n",
    "#use locals to create variable name\n",
    "l=locals()\n",
    "count=1\n",
    "for yu in latlon:\n",
    "    if count==7 or count==12 or count==26:\n",
    "        jh=np.array([13.3, 12.3, 11.3, 10.3])\n",
    "        kj=np.array([ 3.27,  4.27,  5.27,  6.27,  7.27,  8.27,  9.27, 10.27,11.27, 12.27, 13.27, 14.27])\n",
    "        #data2['lat'][:]\n",
    "        for kkhk,g in enumerate(jh):\n",
    "            if str(g)==str(yu[0]):\n",
    "                break;\n",
    "        for mghg,g in enumerate(kj):\n",
    "            if str(g)== str(yu[1]):\n",
    "                break;\n",
    "        df1=[]\n",
    "        for ty in lid1:\n",
    "            df1+=[data1['temp'][ty,kkhk,mghg].item()]\n",
    "        df2=[]\n",
    "        for ty in lid2:\n",
    "            df2+=[data2['temp'][ty,kkhk,mghg].item()]\n",
    "        df3=[]\n",
    "        ttt3=[]\n",
    "        for ty in lid3:\n",
    "            df3+=[data3['temp'][ty,kkhk,mghg].item()]\n",
    "        df4=[]\n",
    "        ttt4=[]\n",
    "        for ty in lid4:\n",
    "            df4+=[data4['temp'][ty,kkhk,mghg].item()]\n",
    "        dtt=pd.DataFrame()\n",
    "        dtt['OBSHI']=df1\n",
    "        dtt['OBS']=df2\n",
    "        dtt['UMO']=df3\n",
    "        dtt['NCEP']=df4\n",
    "        dtt['ECMWF']=df5\n",
    "        dtt['time lead']=po\n",
    "        OBSHW=[]\n",
    "        OBSHW_in=[]\n",
    "        for i in range(len(dtt)):\n",
    "            if dtt['OBSHI'].iloc[i]>0:\n",
    "                k='H'\n",
    "                ku=1\n",
    "                OBSHW+=[k]\n",
    "                OBSHW_in+=[ku]\n",
    "            else:\n",
    "                k='NH'\n",
    "                ku=0\n",
    "                OBSHW+=[k]\n",
    "                OBSHW_in+=[ku]\n",
    "        dtt['OBSHW']=OBSHW\n",
    "        dtt['OBSHW_in']=OBSHW_in\n",
    "        for i in range (3):\n",
    "            dtt=dtt.append(dtt)\n",
    "        X=dtt.drop(['OBS','OBSHW','OBSHI','OBSHW_in'],axis=1)\n",
    "        y=dtt['OBSHW_in']\n",
    "        from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "        scaled=StandardScaler()\n",
    "        scaled.fit(X)\n",
    "        scaled_data=scaled.transform(X)\n",
    "        from sklearn.model_selection import train_test_split,TimeSeriesSplit,GridSearchCV\n",
    "        X_train, X_test, y_train, y_test =train_test_split(scaled_data, y, test_size=0.73)\n",
    "        l['model'+str(int(yu[0]))+str(int(yu[1]))]=Sequential()\n",
    "        l['model'+str(int(yu[0]))+str(int(yu[1]))].add(Dense(64,input_dim=4,activation='relu'))\n",
    "        l['model'+str(int(yu[0]))+str(int(yu[1]))].add(Dense(40,activation='relu'))\n",
    "        l['model'+str(int(yu[0]))+str(int(yu[1]))].add(Dense(32,activation='relu'))\n",
    "        l['model'+str(int(yu[0]))+str(int(yu[1]))].add(Dense(24,activation='relu'))\n",
    "        l['model'+str(int(yu[0]))+str(int(yu[1]))].add(Dense(12,activation='relu'))\n",
    "        l['model'+str(int(yu[0]))+str(int(yu[1]))].add(Dense(6,activation='relu'))\n",
    "        l['model'+str(int(yu[0]))+str(int(yu[1]))].add(Dense(1, activation='sigmoid'))\n",
    "        l['model'+str(int(yu[0]))+str(int(yu[1]))].compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        l['model'+str(int(yu[0]))+str(int(yu[1]))].fit(X_train,y_train,verbose=0,validation_split=0.3,epochs=200,batch_size=15)\n",
    "        pred_data=scaled.transform(X)\n",
    "        l['model'+str(int(yu[0]))+str(int(yu[1]))].evaluate(pred_data,y)\n",
    "        l['model'+str(int(yu[0]))+str(int(yu[1]))].save('model'+str(int(yu[0]))+str(int(yu[1]))+'.h5')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40042990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the meteorological skill score at each grid point\n",
    "auc=[]\n",
    "acc=[]\n",
    "rel=[]\n",
    "anomalyacc=[]\n",
    "sethit=[]\n",
    "falsealarm=[]\n",
    "setmiss=[]\n",
    "correctnegative=[]\n",
    "pred_cum=[]\n",
    "i=0\n",
    "l=locals()\n",
    "for yu in latlon:\n",
    "    jh=np.array([13.3, 12.3, 11.3, 10.3])\n",
    "    kj=np.array([ 3.27,  4.27,  5.27,  6.27,  7.27,  8.27,  9.27, 10.27,11.27, 12.27, 13.27, 14.27])\n",
    "    #data2['lat'][:]\n",
    "    for kkhk,g in enumerate(jh):\n",
    "        if str(g)==str(yu[0]):\n",
    "            break;\n",
    "    for mghg,g in enumerate(kj):\n",
    "        if str(g)== str(yu[1]):\n",
    "            break;\n",
    "    df1=[]\n",
    "    for ty in lid1:\n",
    "        df1+=[data1['temp'][ty,kkhk,mghg].item()]\n",
    "    df2=[]\n",
    "    for ty in lid2:\n",
    "        df2+=[data2['temp'][ty,kkhk,mghg].item()]\n",
    "    df3=[]\n",
    "    ttt3=[]\n",
    "    for ty in lid3:\n",
    "        df3+=[data3['temp'][ty,kkhk,mghg].item()]\n",
    "    df4=[]\n",
    "    ttt4=[]\n",
    "    for ty in lid4:\n",
    "        df4+=[data4['temp'][ty,kkhk,mghg].item()]\n",
    "    dtt=pd.DataFrame()\n",
    "    dtt['OBSHI']=df1\n",
    "    dtt['OBS']=df2\n",
    "    dtt['UMO']=df3\n",
    "    dtt['NCEP']=df4\n",
    "    dtt['ECMWF']=df5\n",
    "    dtt['time lead']=po\n",
    "    OBSHW=[]\n",
    "    OBSHW_in=[]\n",
    "    for i in range(len(dtt)):\n",
    "        if dtt['OBSHI'].iloc[i]>0:\n",
    "            k='H'\n",
    "            ku=1\n",
    "            OBSHW+=[k]\n",
    "            OBSHW_in+=[ku]\n",
    "        else:\n",
    "            k='NH'\n",
    "            ku=0\n",
    "            OBSHW+=[k]\n",
    "            OBSHW_in+=[ku]\n",
    "    dtt['OBSHW']=OBSHW\n",
    "    dtt['OBSHW_in']=OBSHW_in\n",
    "    X=dtt.drop(['OBS','OBSHW','OBSHI','OBSHW_in'],axis=1)\n",
    "    y=dtt['OBSHW_in']\n",
    "    from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "    from tensorflow.keras.models import load_model\n",
    "    scaled=StandardScaler()\n",
    "    scaled.fit(X)\n",
    "    scaled_data=scaled.transform(X)\n",
    "    model=load_model('model'+str(int(yu[0]))+str(int(yu[1]))+'.h5')\n",
    "    pred=model.predict_classes(scaled_data)\n",
    "    from sklearn.metrics import accuracy_score,roc_auc_score,precision_score\n",
    "    auc+=[roc_auc_score(y,pred)]\n",
    "    rel+=[precision_score(y,pred)]\n",
    "    acc+=[accuracy_score(y,pred)]\n",
    "    #select days of heatwave to check for anomally\n",
    "    anomally=pd.DataFrame()\n",
    "    anomally['y']=y\n",
    "    anomally['pred']=pred\n",
    "    ann=anomally[anomally['y']=='H']\n",
    "    anomalyacc+=[accuracy_score(ann['y'],ann['pred'])]\n",
    "    from sklearn.metrics import classification_report,confusion_matrix,plot_confusion_matrix\n",
    "    c_m=confusion_matrix(y,pred)\n",
    "    sethit+=[c_m[1][1]]\n",
    "    falsealarm+=[c_m[0][1]]\n",
    "    setmiss+=[c_m[1][0]]\n",
    "    correctnegative+=[c_m[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2220dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile score into dataframe \n",
    "hr=np.array(sethit)/(np.array(sethit)+np.array(setmiss))\n",
    "fr=np.array(falsealarm)/(np.array(falsealarm)+np.array(correctnegative))\n",
    "r=1-np.array(hr)\n",
    "q=1-np.array(fr)\n",
    "sedi=(np.log(np.array(fr))-np.log(np.array(hr))-np.log(np.array(q))+np.log(np.array(r)))/(np.log(np.array(fr))+np.log(np.array(hr))+np.log(np.array(q))+np.log(np.array(r)))\n",
    "ft=pd.DataFrame()\n",
    "ft['acc']=acc\n",
    "#ft['anomallyacc']=anomalyacc\n",
    "ft['hit']=sethit\n",
    "ft['miss']=setmiss\n",
    "ft['falsealarm']=falsealarm\n",
    "ft['correctnegative']=correctnegative\n",
    "ft['hit rate']=hr\n",
    "ft['false rate']=fr\n",
    "ft['auc']=auc\n",
    "ft['sedi score']=sedi\n",
    "ft['reliability']=rel\n",
    "ft"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
